

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Facial Keypoint Detection with Neural Networks</title>
    <link rel="stylesheet" href="styles.css">
</head>

    
<body>
    <div class="background-container">
        <div class="image-layer image1"></div>
        <div class="image-layer image2"></div>
        <div class="image-layer image3"></div>
    </div>


    <header id="header">
        <div class="title-image">
            <h1>Facial Keypoint Detection with Neural Networks</h1>
        </div>
    </header>


    <section id="Overview">
        <h2>Overview</h2>
        <p>
            use neural networks to automatically detect facial keypoints 
        </p>
    </section>
    



    <section id="naive-methods">
        <h2>Part 1: Nose Tip Detection</h2>

        <p> 
I started by training an initial toy model for detecting a single keypoint. The section is called  nose tip detection but the instructions mentioned detecting pixels at index [-6] which is really more the bottom of the nose. The first step is parsing through the asf files of the IMM Face Database to get the images and their annotations:
            
        </p>


        <h2> Sampled image from dataloader </h2>
        <div class="gallery medium-gallery">   
            <figure>
                <img src="images/1a.png" alt="Image 1">
            </figure>
            <figure>
                <img src="images/1b.png" alt="Image 1">
            </figure>
             <figure>
                <img src="images/1c.png" alt="Image 1">
            </figure>
        </div>

           <p> 
               Our toy model is a 3 layer CNN that uses ReLU for non linearity and a MaxPool between each layer. The model ends in 2 fully connected layers and is trained. 
           </p>

        <h2> MSE loss/model architecture </h2>
        <p>
            Hypperparameter values: 
        </p>

        <div class="gallery medium-gallery">   
            <figure>
                <img src="images/CNN1.png" alt="Image 1">
            </figure>
<figcaption>I used the Adam optimizer with learning rate 1e-3 for 20 epochs</figcaption> 
            <figure>
                <img src="images/1d.png" alt="Image 1">
            </figure>
        </div>


       <p> 
          Testing our model reveals that it tends to do a lot better when the face is closer to the average alignment The model preforms significantly better the more the subject is centered and facing forwards.
       </p>
        
        <h2>Success</h2>
        <div class="gallery medium-gallery">   
            <figure>
                <img src="images/1e.png" alt="Image 1">
            </figure>
            <figure>
                <img src="images/1f.png" alt="Image 1">
            </figure>
            
        </div>

        <h2>Failure</h2>

        <div class="gallery medium-gallery">   
            <figure>
                <img src="images/1g.png" alt="Image 1">
            </figure>
            <figure>
                <img src="images/1h.png" alt="Image 1">
            </figure>
            
        </div>


        
    </section>


    <section id="speed">
        <h2>Part 2: Full Facial Keypoints Detection</h2>

        <p>
I begin this section by implementing a stronger CNN with two additional layers and more careful design/tuning of the hyperparameters (i.e. channels/stride/kernel width) in order to detect all 58 facial keypoints/landmarks. Same dataset as Part 1

        </p>

        <p> 
            Our data loader now has all they keypoints. We also apply some data augmentation to make our database “bigger”. We do this by applying small random translations,rotation, and color jitters to our data in order to create synthetic data. This process was a bit annoying using a torch because it was not trivial to apply the same transformation to our labels, I had to create a custom transformation class. 
        </p>

        <h2> Data Loader/Augmentation </h2>

        <div class="gallery medium-gallery">   
            <figure>
                <img src="images/2a.png" alt="Image 2">
            </figure>
                <figure>
                <img src="images/2b.png" alt="Image 2">
            </figure>
                <figure>
                <img src="images/2c.png" alt="Image 2">
            </figure>

        </div>

        
        <h2>CNN architecture</h2>

        <p>
            The idea is that the CNN grows in channels while being reduced in size. I decidied not to include a MaxPool layer after the first
            Convolutinal layer because I didn't want to downsample that aggresivley in the beggining. Other than that this is eccentially the first model,
            just with more layers and more parameters per some layers.
        </p>
        
        <p>
            Hypperparameter values:
        </p>

        <div class="gallery medium-gallery">   
            <figure>
                <img src="images/CNN2.png" alt="Image 2">
            </figure>
             <figcaption>I used the Adam optimizer with learning rate 1e-3 for 20 epochs</figcaption> 

        </div>

        <h2>Results</h2>

        <p>
            The network preforms fairly well, It still gets thrown off by being off center or having distinghing features in terms of hair glasess etc. 
        </p>

        <div class="gallery medium-gallery">   
            <figure>
                <img src="images/2j.png" alt="Image 2">
            </figure>
                <figure>
                <img src="images/2d.png" alt="Image 2">
            </figure>

        </div>
        
        <h2>Visualizing the Learned Filters</h2>

        <p>
        

These learned filters don’t reveal as much about the image as I would like but they give us a snapshot of what the network “looks at”


        </p>

<div class="gallery small_gallery"> 
    <figure> <img src="images/2e.png" alt="Image 2"> <figcaption>learned filter 1</figcaption> </figure> 
    <figure> <img src="images/2f.png" alt="Image 2"> <figcaption>learned filter  2</figcaption> </figure>
    <figure> <img src="images/2g.png" alt="Image 2"> <figcaption>learned filter  3</figcaption> </figure>
    <figure> <img src="images/2h.png" alt="Image 2"> <figcaption>learned filter  4</figcaption> </figure> 
    <figure> <img src="images/2i.png" alt="Image 2"> <figcaption>learned filter  5</figcaption> </figure> 
</div>
            
    </section>


       <section id="charicatures">
       <h2>Part 3: Train With Larger Dataset </h2>

        <p> 
Now I move onto the larger dataset: ibug face. This dataset has 6666 images of varying image sizes, and each image has 68 annotated facial keypoints. We use the annotations to crop out the face from each image.
        </p>

        <h2> Data Loader </h2>

        <div class="gallery medium-gallery">   
            <figure>
                <img src="images/3a.png" alt="Image 2">
            </figure>
                <figure>
                <img src="images/3b.png" alt="Image 2">
            </figure>
                <figure>
                <img src="images/3c.png" alt="Image 2">
            </figure>

        </div>
           
  
        <h2>CNN architecture</h2>

        <p>
            I used ResNet18 as suggested. I had to modify the first layer to only take one channel (grayscale), and I changed the output layer to be 136 (viewed as  68 * 2 array of coordinates). 

        </p>
        
        <p>
            Hypperparameter values:
        </p>

        <div class="gallery medium-gallery">   
            <figure>
                <img src="images/CNN2.png" alt="Image 2"> <figcaption>I used the Adam optimizer with learning rate 1e-3 for 20 epochs</figcaption> 

            </figure>

                 <figure>
                <img src="images/3d.png" alt="Image 2">
            </figure>

        </div>

        

        <h2>Results</h2>

        <p>
            I would say they the results are quite good, not perfect but the structure seems really good and the points seem quite close.
        </p>

        <div class="gallery medium-gallery">   
 
            <figure>
                <img src="images/3h.png" alt="Image 2">
            </figure>
            <figure>
                <img src="images/3i.png" alt="Image 2">
            </figure>

            <figure>
                <img src="images/3j.png" alt="Image 2">
            </figure>
        </div>


        <h2>Test Set</h2>

        <p>
            
            Without cropping the result is a bit worse

        </p>

        <div class="gallery medium-gallery">   
 
            <figure>
                <img src="images/3e.png" alt="Image 2">
            </figure>
            <figure>
                <img src="images/3f.png" alt="Image 2">
            </figure>

            <figure>
                <img src="images/3g.png" alt="Image 2">
            </figure>
        </div>


    <h2>My images</h2>

        <p>
            
          I'm suprosed it did so well with images  it had never seen before, including animated chaaracters. It did the worst with my face probobly because the turn is too much.

        </p>

        <div class="gallery medium-gallery">   
             <figure>
                <img src="images/3k.png" alt="Image 2">
            </figure>
            <figure>
                <img src="images/3l.png" alt="Image 2">
            </figure>
            <figure>
                <img src="images/3m.png" alt="Image 2">
            </figure>

            <figure>
                <img src="images/3o.png" alt="Image 2">
            </figure>
        </div>
            
            
    </section>


    
    <section id="speed">
        <h2>Inpainting</h2>
      
        <p> 
            Given an image and a binary mask, we can create a new image that has new content wherever the mask covers. we leave everything inside the edit mask alone, but we replace everything outside the edit mask with our original image -- with the correct amount of noise added for timestep t.
        </p>


        
        <div class="gallery large-gallery">
            <figure>
                <img src="images/6a.png" alt="Image 1">
            </figure>
     
        </div>


        <div class="gallery large-gallery">
            <figure>
                <img src="images/6b.png" alt="Image 1">
            </figure>
            <figure>
                <img src="images/6c.png" alt="Image 1">
            </figure>
            <figure>
                <img src="images/6d.png" alt="Image 1">
            </figure>
        </div>


    </section>


<section id="speed">
        <h2>Text-Conditioned Image-to-image Translation</h2>
        <p>
            We can guide the projection of the image with a text prompt, converting thins into rockets for example. noise levels = [1, 3, 5, 7, 10, 20]
        </p>


    <div class="gallery large-gallery">
        <figure>
            <img src="images/7a.png" alt="Image 1">
        </figure>
    </div>
    </section>


<section id="speed">
    <h2>Visual Anagrams</h2>
    <p>
        To create visual anagrams we will denoise an image at step t normally with one prompt and at the same time, we will flip the inmage upside down, and denoise with the prompt a different prompt, to get another noise estimate. We can flip the second noise estimate , to make it right-side up, and average the two noise estimates. We can then perform a reverse diffusion step with the averaged noise estimate.
    </p>
    <p>
        Prompts:
    </p>
        <p>
    an oil painting of an old man + an oil painting of people around a campfire
    </p>
        <p>
a rocket ship + a man wearing a hat
    </p>
    <p>
an oil painting of a snowy mountain village + a lithograph of a skull
    </p>


    
    <div class="gallery medium-gallery">
        <figure>
            <img src="images/8a.png" alt="Image 2">
        </figure>       
        <figure>
            <img src="images/8b.png" alt="Image 2">
        </figure>

        <figure>
            <img src="images/8c.png" alt="Image 2">
        </figure>
    </div>

    <div class="gallery large_gallery">
        <figure>
            <img src="images/8d.png" alt="Image 2">
        </figure>
        <figure>
            <img src="images/8e.png" alt="Image 2">
        </figure>
    </div>


</section>


<section id="speed">
    <h2>Hybrid Images</h2>
    <p>
        Another cool visual ilussion. The proedure is to create a composite noise estimate epsilon by estimating the noise with two different text prompts, and then combining low frequencies from one noise estimate with high frequencies of the other. 
    </p>
    <p>
        Prompts:
    </p>
        <p>
    a lithograph of a skull + a lithograph of waterfalls
    </p>
        <p>
a lithograph of a skull + an oil painting of a snowy mountain village
    </p>
    <p>
    an oil painting of an old man + an oil painting of people around a campfire
    </p>

    <div class="gallery small-gallery">
        <figure>
            <img src="images/9a.png" alt="Image 2">
        </figure>       
        <figure>
            <img src="images/9b.png" alt="Image 2">
        </figure>

        <figure>
            <img src="images/9c.png" alt="Image 2">
        </figure>
    </div>

    <div class="gallery medium-gallery">
    <figure>
        <img src="images/9a.png" alt="Image 2">
    </figure>       
    <figure>
        <img src="images/9b.png" alt="Image 2">
    </figure>

    <figure>
        <img src="images/9c.png" alt="Image 2">
    </figure>
</div>

        <div class="gallery small-gallery">
        <figure>
            <img src="images/9d.png" alt="Image 2">
        </figure>
        
        <figure>
            <img src="images/9e.png" alt="Image 2">
        </figure>
    </div>


    <div class="gallery large_gallery">
        <figure>
            <img src="images/9d.png" alt="Image 2">
        </figure>
        
        <figure>
            <img src="images/9e.png" alt="Image 2">
        </figure>
    </div>



</section>


<section id="speed">
    <h1>PART B</h1>
    <h2>Training a Single-Step Denoising UNet</h2>
    <p>
        Architecture:
    </p>

    <div class="gallery large_gallery">
        <figure>
            <img src="images/10a.png" alt="Image 2">
        </figure>       
    </div>

    <p>We start by building a simple one step denoiser and we implement it as a UNet. Architecture Above.</p>

    <p>
        Visualize the different noising processes over sigma = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]
    </p>

    <div class="gallery large_gallery">
        <figure>
            <img src="images/10b.png" alt="Image 2">
        </figure>
    </div>

    <p>
        The model denoising at epoch 1, and 5 
    </p>

    <div class="gallery large_gallery">
        <figure>
            <img src="images/10c.png" alt="Image 2">
        </figure>
        <figure>
            <img src="images/10d.png" alt="Image 2">
        </figure>
    </div>

    <p>
        Training loss, model de noising over sigma = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0] 
    </p>

    <div class="gallery large_gallery">
        <figure>
            <img src="images/10e.png" alt="Image 2">
        </figure>
        <figure>
            <img src="images/10f.png" alt="Image 2">
        </figure>
    </div>
    <h3>Reflections</h3>
    <p>You can see that in the first epoch that the denoising isn’t perfect and certain edges, intersections and curves seem difficult for the network to parse ( especially in the 4 and the 3) But by the 5th epoch the reconstructions are very convincing. </p>

</section>


<section id="speed">
    <h2>Adding Time Conditioning to UNet</h2>
    <p>
       We need a way to inject scalar t into our UNet model to condition it, for this we do as the project website recommends  and introduce a block called FCBlock that consists of a Linear block followed by a GELU followed by another Linear block.  We have two of these blocks that take in a normalized t and then we inject the output into the up block and the unflatten sections respectively.
    </p>

     <p>
        Results
    </p>

    <div class="gallery large_gallery">
        <figure>
            <img src="images/11a.png" alt="Image 2">
        </figure>      

        <figure>
            <img src="images/11c.png" alt="Image 2">
        </figure>      

        <figure>
            <img src="images/11d.png" alt="Image 2">
        </figure>    
    </div>
 <h3>Reflections</h3>
<p>
This is clearly a more difficult task, it takes more epochs to get good results. In epoch 5 you can recognize numbers but at the same time you can tell that something is off.
</p>

</section>

<section id="speed">
    <h2>Adding Class-Conditioning to UNet</h2>
    <p>
To make the results better and give us more control for image generation, we can also optionally condition our UNet on the class of the digit 0-9. We do this by adding two more FCBlocks and then by injecting their output in the way that the project description recommends.     </p>

     <p>
        Results
    </p>

    <div class="gallery large_gallery">
        <figure>
            <img src="images/12a.png" alt="Image 2">
        </figure>      

        <figure>
            <img src="images/12c.png" alt="Image 2">
        </figure>      

        <figure>
            <img src="images/12d.png" alt="Image 2">
        </figure>    
    </div>
 <h3>Reflections</h3>

        <p>
Both epoch 5 and 20 create the distinct numbers we want in the right locations, epoch 5 still creates some small artifacts and separations (the fours and nines are good examples, but this might be because some people write their nines and fours with that point). Once we’ve reached epoch 20 all those imperfections have been smoothed out,    </p>

     <p>
</section>


<section id="additionals">
    <h2>Coolest thing I learned</h2>
    <p>
    Clasifier free guidance was very interesting, using the models to fill in gaps was also cool but I think the coolest thing I learnt was architecture for u net
    </p>
</section>


<section id="additionals">
    <h2>Information</h2>
    <p>
        This website contains transitions not captured by the pdf, spesificaly, the title image changes into a high gamma verison and then into the black and white threshold filter version.
    </p>
</section>

    <section id="additionals">
    <h2>Sources</h2>
    <p>
        https://cal-cs180.github.io/fa24/hw/proj5/partb.html
    </p>
            <p>
https://cal-cs180.github.io/fa24/hw/proj5/parta.html
            </p>
</section>


    <script src="script.js"></script>
</body>

<script src="script.js"></script>
</html>
