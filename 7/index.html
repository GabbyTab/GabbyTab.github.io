

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Facial Keypoint Detection with Neural Networks</title>
    <link rel="stylesheet" href="styles.css">
</head>

    
<body>
    <div class="background-container">
        <div class="image-layer image1"></div>
        <div class="image-layer image2"></div>
        <div class="image-layer image3"></div>
    </div>


    <header id="header">
        <div class="title-image">
            <h1>Facial Keypoint Detection with Neural Networks</h1>
        </div>
    </header>


    <section id="Overview">
        <h1>Facial Keypoint Detection with Neural Networks</h1>
        <h2>Overview</h2>
        <p>
            use neural networks to automatically detect facial keypoints.
        </p>
    </section>
    



    <section id="naive-methods">
        <h2>Part 1: Nose Tip Detection</h2>

        <p> 
            I started by training an initial toy model for detecting a single keypoint. The section is called nose tip detection but the instructions mentioned detecting pixels at index [-6] which is really more the bottom of the nose. The first step is parsing through the asf files of the IMM Face Database to get the images and their annotations:
        </p>


        <h2> Sampled image from dataloader </h2>
        <div class="gallery large-gallery">   
            <figure>
                <img src="images/1a.png" alt="Image 1">
            </figure>
            <figure>
                <img src="images/1b.png" alt="Image 1">
            </figure>
             <figure>
                <img src="images/1c.png" alt="Image 1">
            </figure>
        </div>

           <p> 
               Our toy model is a 3 layer CNN that uses ReLU for non linearity and a MaxPool between each layer. The model ends in 2 fully connected layers and is trained. 
           </p>

        <h2> MSE loss/model architecture </h2>
        <p>
            Hyperparameter values:
        </p>

        <div class="gallery medium-gallery">   
            <figure>
                <img src="images/CNN1.png" alt="Image 1">
            </figure>
<figcaption>I used the Adam optimizer with learning rate 1e-3 for 20 epochs</figcaption> 
            <figure>
                <img src="images/1d.png" alt="Image 1">
            </figure>
        </div>


       <p> 
           Testing our model reveals that it tends to do a lot better when the face is closer to the average alignment. The model performs significantly better the more the subject is centered and facing forwards.
       </p>
        
        <h2>Success</h2>
        <div class="gallery large-gallery">   
            <figure>
                <img src="images/1e.png" alt="Image 1">
            </figure>
            <figure>
                <img src="images/1f.png" alt="Image 1">
            </figure>
            
        </div>

        <h2>Failure</h2>

        <div class="gallery large-gallery">   
            <figure>
                <img src="images/1g.png" alt="Image 1">
            </figure>
            <figure>
                <img src="images/1h.png" alt="Image 1">
            </figure>
            
        </div>


        
    </section>


    <section id="speed">
        <h2>Part 2: Full Facial Keypoints Detection</h2>

        <p>
I begin this section by implementing a stronger CNN with two additional layers and more careful design/tuning of the hyperparameters (i.e. channels/stride/kernel width) in order to detect all 58 facial keypoints/landmarks. Same dataset as Part 1

        </p>

        <p> 
            Our data loader now has all these key points. We also apply some data augmentation to make our database “bigger”. We do this by applying small random translations,rotation, and color jitters to our data in order to create synthetic data. This process was a bit annoying using a torch because it was not trivial to apply the same transformation to our labels, I had to create a custom transformation class. 

        </p>

        <h2> Data Loader/Augmentation </h2>

        <div class="gallery large-gallery">   
            <figure>
                <img src="images/2a.png" alt="Image 2">
            </figure>
                <figure>
                <img src="images/2b.png" alt="Image 2">
            </figure>
                <figure>
                <img src="images/2c.png" alt="Image 2">
            </figure>

        </div>

        
        <h2>CNN architecture</h2>

        <p>
            The idea is that the CNN grows in channels while being reduced in size. I decided not to include a MaxPool layer after the first Convolutional layer because I didn't want to downsample that aggressively in the beginning. Other than that this is essentially the first model, just with more layers and more parameters per some layers.
        </p>
        
        <p>
            Hyperparameter values:
        </p>

        <div class="gallery medium-gallery">   
            <figure>
                <img src="images/CNN2.png" alt="Image 2">
            </figure>
            
             <figcaption>I used the Adam optimizer with learning rate 1e-3 for 20 epochs</figcaption> 
           <figure>
                <img src="images/2j.png" alt="Image 2">
            </figure>
        </div>

        <h2>Results</h2>

        <p>
The network performs fairly well, It still gets thrown off by being off center or having distinguishing features in terms of hair glasses etc. 
        </p>

        <div class="gallery medium-gallery">   
 
                <figure>
                <img src="images/2d.png" alt="Image 2">
            </figure>

        </div>
        
        <h2>Visualizing the Learned Filters</h2>

        <p>
        

These learned filters don’t reveal as much about the image as I would like but they give us a snapshot of what the network “looks at”


        </p>

<div class="gallery small_gallery"> 
    <figure> <img src="images/2e.png" alt="Image 2"> <figcaption>learned filter 1</figcaption> </figure> 
    <figure> <img src="images/2f.png" alt="Image 2"> <figcaption>learned filter  2</figcaption> </figure>
    <figure> <img src="images/2g.png" alt="Image 2"> <figcaption>learned filter  3</figcaption> </figure>
    <figure> <img src="images/2h.png" alt="Image 2"> <figcaption>learned filter  4</figcaption> </figure> 
    <figure> <img src="images/2i.png" alt="Image 2"> <figcaption>learned filter  5</figcaption> </figure> 
</div>
            
    </section>


       <section id="charicatures">
       <h2>Part 3: Train With Larger Dataset </h2>

        <p> 
Now I move onto the larger dataset: ibug face. This dataset has 6666 images of varying image sizes, and each image has 68 annotated facial keypoints. We use the annotations to crop out the face from each image.
        </p>

        <h2> Data Loader </h2>

        <div class="gallery large-gallery">   
            <figure>
                <img src="images/3a.png" alt="Image 2">
            </figure>
                <figure>
                <img src="images/3b.png" alt="Image 2">
            </figure>
                <figure>
                <img src="images/3c.png" alt="Image 2">
            </figure>

        </div>
           
  
        <h2>CNN architecture</h2>

        <p>
            I used ResNet18 as suggested. I had to modify the first layer to only take one channel (grayscale), and I changed the output layer to be 136 (viewed as  68 * 2 array of coordinates). 

        </p>
        
        <p>
            Hypperparameter values:
        </p>

        <div class="gallery large-gallery">   
            <figure>
                <img src="images/CNN3.png" alt="Image 2"> 

            </figure>

            <figure>
                <img src="images/3d.png" alt="Image 2">
            </figure>

        </div>

        <p>I used the Adam optimizer with learning rate 1e-3 for 20 epochs </p>

        <h2>Results</h2>

        <p>
            I would say the results are quite good, not perfect but the structure seems really good and the points seem quite close.
        </p>

        <div class="gallery large-gallery">   
 
            <figure>
                <img src="images/3h.png" alt="Image 2">
            </figure>
            <figure>
                <img src="images/3i.png" alt="Image 2">
            </figure>

            <figure>
                <img src="images/3j.png" alt="Image 2">
            </figure>
        </div>


        <h2>Test Set</h2>

        <p>
            Without cropping the result is a bit worse
        </p>

        <div class="gallery large-gallery">   
 
            <figure>
                <img src="images/3e.png" alt="Image 2">
            </figure>
            <figure>
                <img src="images/3f.png" alt="Image 2">
            </figure>

            <figure>
                <img src="images/3g.png" alt="Image 2">
            </figure>
        </div>


    <h2>My images</h2>

        <p>
            
 I'm surprised it did so well with images  it had never seen before, including animated characters. It did the worst with my face probably because the turn is too much.
        </p>

        <div class="gallery medium-gallery">   
             <figure>
                <img src="images/3k.png" alt="Image 2">
            </figure>
            <figure>
                <img src="images/3l.png" alt="Image 2">
            </figure>
            <figure>
                <img src="images/3m.png" alt="Image 2">
            </figure>

            <figure>
                <img src="images/3o.png" alt="Image 2">
            </figure>
        </div>
            
            
    </section>


    
    <section id="speed">
     <h2>Part 4: Pixelwise Classification</h2>

        <p> 
  Now we're doing pixel wise classification. For each key point we make a 224 by 224 grid and center a gaussian on the location of the point with epsilon = 3. This is what our model is trying to predict/learn.

        </p>

        <h2> Data Loader </h2>

        <div class="gallery large-gallery">   
            <figure>
                <img src="images/4a.png" alt="Image 2">
            </figure>
                <figure>
                <img src="images/4b.png" alt="Image 2">
            </figure>
                <figure>
                <img src="images/4c.png" alt="Image 2">
            </figure>

        </div>
           
  
        <h2>CNN architecture</h2>

        <p>
            I used ResNet50 as suggested. I again had to change the input output layers for the Model to be comatible.  

        </p>
        
        <p>
            Hypperparameter values:
        </p>

        <div class="gallery large-gallery">   
            <figure>
                <img src="images/CNN4.png" alt="Image 2">

            </figure>
             <figcaption>I used the Adam optimizer with learning rate 1e-3 for 20 epochs, and epsilon = 3</figcaption> 
            <figure>
                <img src="images/4d.png" alt="Image 2">
            </figure>

        </div>

        

        <h2>Results</h2>

        <p>
            Even better results than before.
        </p>

        <div class="gallery large-gallery">   
 
            <figure>
                <img src="images/4h.png" alt="Image 2">
            </figure>
            <figure>
                <img src="images/4i.png" alt="Image 2">
            </figure>

            <figure>
                <img src="images/4j.png" alt="Image 2">
            </figure>
        </div>


        <h2>Test Set</h2>


        <div class="gallery large-gallery">   
 
            <figure>
                <img src="images/4e.png" alt="Image 2">
            </figure>
            <figure>
                <img src="images/4f.png" alt="Image 2">
            </figure>

            <figure>
                <img src="images/4g.png" alt="Image 2">
            </figure>
        </div>


    <h2>My images</h2>

        <p>
            Unlike part 3, this one is actually able to annotate my face.
        </p>

        <div class="gallery large-gallery">   
             <figure>
                <img src="images/4o.png" alt="Image 2">
            </figure>
            <figure>
                <img src="images/4l.png" alt="Image 2">
            </figure>
            <figure>
                <img src="images/4m.png" alt="Image 2">
            </figure>

            <figure>
                <img src="images/4k.png" alt="Image 2">
            </figure>
        </div>
            
            
    </section>



<section id="speed">
    <h2>BW</h2>
    <p> Using 1 and 0 mask heatmaps for the landmarks instead of Gaussian.</p>
    <p> I added a flag to my heat map generating function so that if sigma = 0 it applies a heat map</p>
    
    <h2> Data Loader for binary heatmaps</h2>
    <div class="gallery medium-gallery">   
        <figure>
            <img src="images/5a.png" alt="Image 2">
        </figure>
            <figure>
            <img src="images/5b.png" alt="Image 2">
        </figure>
            <figure>
            <img src="images/5c.png" alt="Image 2">
        </figure>

    </div>


</section>


<section id="additionals">
    <h2>Coolest thing I learned</h2>
    <p>
        The different architectures I learned about were pretty cool, (Like Resnet). However Being able to do one of my least favorite parts of project 3 automatically beats everything.
    </p>
</section>


<section id="additionals">
    <h2>Information</h2>
    <p>
        This website contains transitions not captured by the pdf, the title image changes into a high gamma verison and then into the black and white threshold filter version.
    </p>
</section>

    <section id="additionals">
    <h2>Sources</h2>
    <p>
https://inst.eecs.berkeley.edu/~cs194-26/fa22/hw/proj5/
    </p>
          
</section>


    <script src="script.js"></script>
</body>

<script src="script.js"></script>
</html>
